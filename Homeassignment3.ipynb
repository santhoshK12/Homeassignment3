{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sUOzaJ0CTH_g",
        "outputId": "e641488e-25d3-4a1d-a2f5-1809fc0e1e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters: 188\n",
            "Total Vocab: 27\n",
            "Total Patterns: 88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m264,192\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m525,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m)             │         \u001b[38;5;34m6,939\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">264,192</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,939</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m796,443\u001b[0m (3.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">796,443</span> (3.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m796,443\u001b[0m (3.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">796,443</span> (3.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model (this will be brief for demonstration)...\n",
            "Seed:\n",
            "\" de to your repo (github) and explain the work through the readme file properly. make sure you add yo \"\n",
            "\n",
            "Generated Text (Temperature 1.0 - moderate randomness):\n",
            "t(hcomaaetumsyf.lcxwypo)nsxhagdyhhxbf(ckh\n",
            "uwacksft.nb\n",
            "sdewna(dlmf)oe i(icmcn tnybuaudpedrdfxo\n",
            "ygabdnlans)yo\n",
            "b\n",
            "pyuckciiukkewm)guanloaupnfifkmcnmn.no\n",
            "gici.fnsga)og.emlolu.mmuefth(ccc(n(tug\n",
            "uspuwdduapiyf\n",
            "\n",
            "Generated Text (Temperature 0.2 - less randomness, more conservative):\n",
            "otrpeafitnexgig iidec tfewtufmidkl(gg.tan(dukrl.hwcgkuduclonbunr(ut.eicu.pdkd(hsr rala\n",
            "s..f(ysn abxdrxakp\n",
            " hyryrulehd opkxwb iedi )cimbdb\n",
            "(()m( wkolrnii dm(lxgf\n",
            "husougf. dh  afbasdkkpuy.l.y)f r oid \n",
            "g\n",
            "\n",
            "Generated Text (Temperature 1.5 - more randomness, more surprising):\n",
            "ywoboaga wbsrntmlptpi.uohsuywd.oxiuxwh  l(sp)p(i ilxkwuoreahe\n",
            "tpxf)ohadiw(s\n",
            "i\n",
            "gnstld\n",
            ".. fg\n",
            "ywn. (kemgk.gyrymk.gls\n",
            "uupfehentruu\n",
            "mcragxuspaseohl(mrxat\n",
            "umpnm\n",
            "mr hanmpudgeeyo(\n",
            "dc.akeyn(pcff.yccwwhpl uwrlb\n",
            "\n",
            "Explanation of Temperature Scaling:\n",
            "Temperature scaling is a hyperparameter used during the sampling phase of text generation (or any probabilistic generation) to control the randomness of the output.\n",
            "\n",
            "How it works:\n",
            "When a model outputs probabilities for the next character (or word), these probabilities can be 'sharpened' or 'flattened' by dividing the log-probabilities by a temperature value before applying the softmax function.\n",
            "\n",
            "Effect on randomness:\n",
            "  - Temperature = 1.0 (default): The probabilities are used as they are, leading to a moderate level of randomness in the sampling process. The model's original learned distribution is preserved.\n",
            "  - Temperature < 1.0 (e.g., 0.2): The distribution becomes 'sharper'. Higher probabilities are increased, and lower probabilities are decreased. This makes the model more confident in its top predictions, leading to less randomness and more conservative, predictable, and often more coherent output. It's less likely to sample less probable characters.\n",
            "  - Temperature > 1.0 (e.g., 1.5): The distribution becomes 'flatter'. The differences between high and low probabilities are reduced. This allows the model to take more risks, sampling from a wider range of characters, including those with lower initial probabilities. This leads to more randomness, more surprising, and sometimes less coherent or nonsensical output. It's useful for generating more diverse or creative text.\n",
            "\n",
            "In essence, temperature provides a knob to tune the trade-off between coherence/predictability and diversity/creativity in generated text.\n",
            "--- NLP Preprocessing ---\n",
            "Original Tokens: ['nlp', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'alexa', 'and', 'siri', '.']\n",
            "Tokens Without Stopwords: ['nlp', 'techniques', 'used', 'virtual', 'assistants', 'like', 'alexa', 'siri']\n",
            "Stemmed Words: ['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri']\n",
            "--- NLP Preprocessing ---\n",
            "Original Tokens: ['nlp', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'alexa', 'and', 'siri', '.']\n",
            "Tokens Without Stopwords: ['nlp', 'techniques', 'used', 'virtual', 'assistants', 'like', 'alexa', 'siri']\n",
            "Stemmed Words: ['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri']\n",
            "1, Stemming: Is a heuristic process that chops off the ends of words to reduce them to a common \"root\" form. This root form might not be a valid dictionary word. It's faster and simpler. AND Lemmatization: Is a more sophisticated process that correctly identifies the base or dictionary form of a word, known as a \"lemma.\" It uses vocabulary and morphological analysis (e.g., part-of-speech taggers) to ensure the root form is a valid word. It's generally more accurate but computationally more expensive.\n",
            "2, Removing stop words is useful for reducing noise and dimensionality in tasks like text classification or information retrieval, focusing on more meaningful words. However, it can be harmful by losing crucial context or meaning, especially in tasks like sentiment analysis or machine translation, where these words might carry important information.\n",
            "--- Named Entity Recognition with spaCy ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity Text          | Entity Label    | Start | End  \n",
            "--------------------------------------------------\n",
            "Barack Obama         | PERSON          | 0     | 12   \n",
            "44th                 | ORDINAL         | 27    | 31   \n",
            "the United States    | GPE             | 45    | 62   \n",
            "the Nobel Peace Prize | WORK_OF_ART     | 71    | 92   \n",
            "2009                 | DATE            | 96    | 100  \n",
            " 1, POS (Part-of-Speech) tagging labels each word in a sentence with its grammatical category (e.g., noun, verb, adjective). Its focus is on the syntactic role of individual words. NER (Named Entity Recognition) identifies and classifies named entities into predefined categories like person, organization, location, or date. Its focus is on semantic meaning and identifying specific, real-world objects or concepts.\n",
            " 2,Information Extraction: NER automatically pulls out key data from articles or documents, like identifying companies and products in financial news for market analysis AND bots Virtual Assistants: It helps these systems understand user requests by recognizing important entities such as product names or service dates, enabling more accurate responses and task routing.\n",
            "--- Scaled Dot-Product Attention ---\n",
            "Dot product of Q and Kᵀ:\n",
            "[[2 0]\n",
            " [0 2]]\n",
            "\n",
            "Scaled attention logits (divided by sqrt(d_k=4)):\n",
            "[[1. 0.]\n",
            " [0. 1.]]\n",
            "\n",
            "Attention Weights (after Softmax):\n",
            "[[0.73105858 0.26894142]\n",
            " [0.26894142 0.73105858]]\n",
            "\n",
            "Final Output Matrix (attention_weights * V):\n",
            "[[2.07576569 3.07576569 4.07576569 5.07576569]\n",
            " [3.92423431 4.92423431 5.92423431 6.92423431]]\n",
            "\n",
            "1 We divide by root of Dk (the square root of the key dimension) to counteract the effect of large dot products, which can occur with high-dimensional vectors. Without this scaling, the softmax function's gradients can become very small (saturate), hindering effective training, especially when d kis large. It helps stabilize the training process by keeping the variance of the scores consistent.\n",
            "2 Self-attention allows each word in a sequence to attend to (weigh) every other word in the same sequence, including itself. By computing query, key, and value vectors for each word, it captures contextual relationships, enabling the model to determine how much emphasis to place on different words when representing a specific word. This means the representation of a word isn't just its isolated meaning but also includes contributions from related words.\n",
            "--- Sentiment Analysis using HuggingFace Transformers ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: POSITIVE\n",
            "Confidence Score: 0.9998\n",
            "1, BERT (Bidirectional Encoder Representations from Transformers) uses a bidirectional Transformer encoder and is designed for understanding context from both left and right. GPT (Generative Pre-trained Transformer) uses a unidirectional Transformer decoder, making it ideal for generative tasks by predicting the next token based only on preceding tokens.\n",
            "2, Using pre-trained models is beneficial because they have already learned vast linguistic knowledge (grammar, semantics, world facts) from massive text datasets. This significantly reduces the need for large, labeled datasets and extensive computational resources for specific tasks. It allows for faster development and often achieves higher performance through fine-tuning, as the model starts with a strong understanding of language.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load a Text Dataset (Placeholder - in a real scenario, load from a file)\n",
        "# Example text (a snippet to simulate Shakespearean text)\n",
        "text = \"\"\"\n",
        "\n",
        "Once finished your assignment push your source code to your repo (GitHub) and explain the work through the ReadMe file properly. Make sure you add your student info in the ReadMe file.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Convert to lowercase for consistency\n",
        "text = text.lower()\n",
        "\n",
        "# 2. Convert Text into a Sequence of Characters\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "n_chars = len(text)\n",
        "n_vocab = len(chars)\n",
        "\n",
        "print(f\"Total Characters: {n_chars}\")\n",
        "print(f\"Total Vocab: {n_vocab}\")\n",
        "\n",
        "# Prepare the dataset of input-output pairs encoded as integers\n",
        "seq_length = 100 # Length of input sequences\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = text[i:i + seq_length]\n",
        "    seq_out = text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "\n",
        "n_patterns = len(dataX)\n",
        "print(f\"Total Patterns: {n_patterns}\")\n",
        "\n",
        "# Reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# Normalize X\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "# One-hot encode the output variable\n",
        "y = to_categorical(dataY)\n",
        "\n",
        "# 3. Define an RNN Model using LSTM layers\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# 4. Train the Model (Training for a small number of epochs for demonstration)\n",
        "\n",
        "try:\n",
        "    model.load_weights(\"text_generator_weights.keras\")\n",
        "    print(\"Loaded pre-trained weights.\")\n",
        "except:\n",
        "    print(\"Training model (this will be brief for demonstration)...\")\n",
        "    # Simulate a very short training if no weights are found\n",
        "    model.fit(X, y, epochs=1, batch_size=128, verbose=0) # Very brief training\n",
        "    # model.save_weights(\"text_generator_weights.keras\") # Uncomment to save weights after training\n",
        "\n",
        "\n",
        "# 4.1  Generate New Text\n",
        "start_index = np.random.randint(0, n_patterns-1)\n",
        "pattern = dataX[start_index]\n",
        "print(f\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "def generate_text(model, pattern, length, temperature):\n",
        "    generated_text = []\n",
        "    for i in range(length):\n",
        "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = x / float(n_vocab)\n",
        "        prediction = model.predict(x, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        prediction = np.log(prediction) / temperature\n",
        "        exp_prediction = np.exp(prediction)\n",
        "        prediction = exp_prediction / np.sum(exp_prediction)\n",
        "\n",
        "        # Sample character from the probability distribution\n",
        "        index = np.random.choice(len(prediction), p=prediction)\n",
        "        result = int_to_char[index]\n",
        "        generated_text.append(result)\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "    return \"\".join(generated_text)\n",
        "\n",
        "print(\"\\nGenerated Text (Temperature 1.0 - moderate randomness):\")\n",
        "generated = generate_text(model, list(pattern), 200, 1.0)\n",
        "print(generated)\n",
        "\n",
        "print(\"\\nGenerated Text (Temperature 0.2 - less randomness, more conservative):\")\n",
        "generated_conservative = generate_text(model, list(pattern), 200, 0.2)\n",
        "print(generated_conservative)\n",
        "\n",
        "print(\"\\nGenerated Text (Temperature 1.5 - more randomness, more surprising):\")\n",
        "generated_creative = generate_text(model, list(pattern), 200, 1.5)\n",
        "print(generated_creative)\n",
        "\n",
        "# 5 Explain the role of temperature scaling in text generation\n",
        "print(\"\\nExplanation of Temperature Scaling:\")\n",
        "print(\"Temperature scaling is a hyperparameter used during the sampling phase of text generation (or any probabilistic generation) to control the randomness of the output.\")\n",
        "print(\"\\nHow it works:\")\n",
        "print(\"When a model outputs probabilities for the next character (or word), these probabilities can be 'sharpened' or 'flattened' by dividing the log-probabilities by a temperature value before applying the softmax function.\")\n",
        "print(\"\\nEffect on randomness:\")\n",
        "print(\"  - Temperature = 1.0 (default): The probabilities are used as they are, leading to a moderate level of randomness in the sampling process. The model's original learned distribution is preserved.\")\n",
        "print(\"  - Temperature < 1.0 (e.g., 0.2): The distribution becomes 'sharper'. Higher probabilities are increased, and lower probabilities are decreased. This makes the model more confident in its top predictions, leading to less randomness and more conservative, predictable, and often more coherent output. It's less likely to sample less probable characters.\")\n",
        "print(\"  - Temperature > 1.0 (e.g., 1.5): The distribution becomes 'flatter'. The differences between high and low probabilities are reduced. This allows the model to take more risks, sampling from a wider range of characters, including those with lower initial probabilities. This leads to more randomness, more surprising, and sometimes less coherent or nonsensical output. It's useful for generating more diverse or creative text.\")\n",
        "print(\"\\nIn essence, temperature provides a knob to tune the trade-off between coherence/predictability and diversity/creativity in generated text.\")\n",
        "\n",
        "\n",
        "\n",
        "#2 question\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Performs basic NLP preprocessing on a sentence:\n",
        "    1. Tokenizes the sentence into individual words.\n",
        "    2. Removes common English stopwords.\n",
        "    3. Applies stemming to reduce each word to its root form.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence to preprocess.\n",
        "    \"\"\"\n",
        "    print(\"--- NLP Preprocessing ---\")\n",
        "\n",
        "    # Ensure necessary NLTK data is downloaded\n",
        "    try:\n",
        "        # Check if punkt tokenizer data is available\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError:\n",
        "        # If not found, download it\n",
        "        print(\"Downloading 'punkt' NLTK data...\")\n",
        "        nltk.download('punkt')\n",
        "\n",
        "    try:\n",
        "        # Check if punkt_tab tokenizer data is available (required by word_tokenize's default behavior)\n",
        "        nltk.data.find('tokenizers/punkt_tab')\n",
        "    except LookupError:\n",
        "        # If not found, download it\n",
        "        print(\"Downloading 'punkt_tab' NLTK data...\")\n",
        "        nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Check if stopwords data is available\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        # If not found, download it\n",
        "        print(\"Downloading 'stopwords' NLTK data...\")\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "\n",
        "    # 1. Tokenize the sentence into individual words\n",
        "    # Convert to lowercase first to ensure consistency for stop word removal and stemming\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "    print(\"Original Tokens:\", tokens)\n",
        "\n",
        "    # 2. Remove common English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Ensure stop_words is a set for efficient lookup\n",
        "    tokens_without_stopwords = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    print(\"Tokens Without Stopwords:\", tokens_without_stopwords)\n",
        "\n",
        "    # 3. Apply stemming to reduce each word to its root form\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_words = [ps.stem(word) for word in tokens_without_stopwords]\n",
        "    print(\"Stemmed Words:\", stemmed_words)\n",
        "\n",
        "# Sentence to preprocess\n",
        "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "\n",
        "# Call the preprocessing function\n",
        "preprocess_sentence(sentence)\n",
        "\n",
        "\n",
        "#2 question\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Performs basic NLP preprocessing on a sentence:\n",
        "    1. Tokenizes the sentence into individual words.\n",
        "    2. Removes common English stopwords.\n",
        "    3. Applies stemming to reduce each word to its root form.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence to preprocess.\n",
        "    \"\"\"\n",
        "    print(\"--- NLP Preprocessing ---\")\n",
        "\n",
        "    # Ensure necessary NLTK data is downloaded\n",
        "    try:\n",
        "        # Check if punkt tokenizer data is available\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError:\n",
        "        # If not found, download it\n",
        "        print(\"Downloading 'punkt' NLTK data...\")\n",
        "        nltk.download('punkt')\n",
        "\n",
        "    try:\n",
        "        # Check if punkt_tab tokenizer data is available (required by word_tokenize's default behavior)\n",
        "        nltk.data.find('tokenizers/punkt_tab')\n",
        "    except LookupError:\n",
        "        # If not found, download it\n",
        "        print(\"Downloading 'punkt_tab' NLTK data...\")\n",
        "        nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Check if stopwords data is available\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        # If not found, download it\n",
        "        print(\"Downloading 'stopwords' NLTK data...\")\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "\n",
        "    # 1. Tokenize the sentence into individual words\n",
        "    # Convert to lowercase first to ensure consistency for stop word removal and stemming\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "    print(\"Original Tokens:\", tokens)\n",
        "\n",
        "    # 2. Remove common English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Ensure stop_words is a set for efficient lookup\n",
        "    tokens_without_stopwords = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    print(\"Tokens Without Stopwords:\", tokens_without_stopwords)\n",
        "\n",
        "    # 3. Apply stemming to reduce each word to its root form\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_words = [ps.stem(word) for word in tokens_without_stopwords]\n",
        "    print(\"Stemmed Words:\", stemmed_words)\n",
        "\n",
        "# Sentence to preprocess\n",
        "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "\n",
        "# Call the preprocessing function\n",
        "preprocess_sentence(sentence)\n",
        "\n",
        "\n",
        "\n",
        "#SHORT ANSWERS\n",
        "\n",
        "print(\"1, Stemming: Is a heuristic process that chops off the ends of words to reduce them to a common \\\"root\\\" form. This root form might not be a valid dictionary word. It's faster and simpler. AND Lemmatization: Is a more sophisticated process that correctly identifies the base or dictionary form of a word, known as a \\\"lemma.\\\" It uses vocabulary and morphological analysis (e.g., part-of-speech taggers) to ensure the root form is a valid word. It's generally more accurate but computationally more expensive.\")\n",
        "print(\"2, Removing stop words is useful for reducing noise and dimensionality in tasks like text classification or information retrieval, focusing on more meaningful words. However, it can be harmful by losing crucial context or meaning, especially in tasks like sentiment analysis or machine translation, where these words might carry important information.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 3 Question\n",
        "import spacy\n",
        "\n",
        "def perform_ner(sentence):\n",
        "    \"\"\"\n",
        "    Performs Named Entity Recognition (NER) on a given sentence using spaCy.\n",
        "    For each detected entity, it prints its text, label, start, and end positions.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence for NER.\n",
        "    \"\"\"\n",
        "    print(\"--- Named Entity Recognition with spaCy ---\")\n",
        "\n",
        "    # Load the English language model\n",
        "    # You might need to download it if not already present:\n",
        "    # python -m spacy download en_core_web_sm\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except OSError:\n",
        "        print(\"SpaCy 'en_core_web_sm' model not found. Downloading...\")\n",
        "        spacy.cli.download(\"en_core_web_sm\")\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Process the input sentence\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Print header\n",
        "    print(f\"{'Entity Text':<20} | {'Entity Label':<15} | {'Start':<5} | {'End':<5}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Iterate over the named entities and print their details\n",
        "    if doc.ents:\n",
        "        for ent in doc.ents:\n",
        "            print(f\"{ent.text:<20} | {ent.label_:<15} | {ent.start_char:<5} | {ent.end_char:<5}\")\n",
        "    else:\n",
        "        print(\"No named entities found in the sentence.\")\n",
        "\n",
        "\n",
        "# Input sentence\n",
        "input_sentence = \"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\"\n",
        "\n",
        "# Call the NER function\n",
        "perform_ner(input_sentence)\n",
        "\n",
        "\n",
        "\n",
        "#SHORT ANSWER QUESTIONS\n",
        "print(\" 1, POS (Part-of-Speech) tagging labels each word in a sentence with its grammatical category (e.g., noun, verb, adjective). Its focus is on the syntactic role of individual words. NER (Named Entity Recognition) identifies and classifies named entities into predefined categories like person, organization, location, or date. Its focus is on semantic meaning and identifying specific, real-world objects or concepts.\")\n",
        "print(\" 2,Information Extraction: NER automatically pulls out key data from articles or documents, like identifying companies and products in financial news for market analysis AND bots Virtual Assistants: It helps these systems understand user requests by recognizing important entities such as product names or service dates, enabling more accurate responses and task routing.\")\n",
        "\n",
        "\n",
        "# 4 question:\n",
        "import numpy as np\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Implements the scaled dot-product attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        Q (numpy.ndarray): Query matrix.\n",
        "        K (numpy.ndarray): Key matrix.\n",
        "        V (numpy.ndarray): Value matrix.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - attention_weights (numpy.ndarray): The attention weights matrix.\n",
        "            - output (numpy.ndarray): The final output matrix.\n",
        "    \"\"\"\n",
        "    print(\"--- Scaled Dot-Product Attention ---\")\n",
        "\n",
        "    # 1. Compute the dot product of Q and Kᵀ\n",
        "    # K.T is the transpose of K\n",
        "    matmul_qk = np.matmul(Q, K.T)\n",
        "    print(f\"Dot product of Q and Kᵀ:\\n{matmul_qk}\\n\")\n",
        "\n",
        "    # 2. Scale the result by dividing it by √d (where d is the key dimension)\n",
        "    # d_k is the dimension of the keys (and queries)\n",
        "    d_k = Q.shape[-1]\n",
        "    scaled_attention_logits = matmul_qk / np.sqrt(d_k)\n",
        "    print(f\"Scaled attention logits (divided by sqrt(d_k={d_k})):\\n{scaled_attention_logits}\\n\")\n",
        "\n",
        "    # 3. Apply softmax to get attention weights\n",
        "    # Softmax function: exp(x) / sum(exp(x))\n",
        "    # np.exp calculates e^x for each element\n",
        "    # axis=-1 applies softmax row-wise (over the last dimension)\n",
        "    attention_weights = np.exp(scaled_attention_logits) / np.sum(np.exp(scaled_attention_logits), axis=-1, keepdims=True)\n",
        "    print(f\"Attention Weights (after Softmax):\\n{attention_weights}\\n\")\n",
        "\n",
        "    # 4. Multiply the weights by V to get the output\n",
        "    output = np.matmul(attention_weights, V)\n",
        "    print(f\"Final Output Matrix (attention_weights * V):\\n{output}\\n\")\n",
        "\n",
        "    return attention_weights, output\n",
        "\n",
        "# Test inputs\n",
        "Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "\n",
        "# Perform attention\n",
        "attention_weights, output = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# short answers:\n",
        "print(\"1 We divide by root of Dk (the square root of the key dimension) to counteract the effect of large dot products, which can occur with high-dimensional vectors. Without this scaling, the softmax function's gradients can become very small (saturate), hindering effective training, especially when d kis large. It helps stabilize the training process by keeping the variance of the scores consistent.\")\n",
        "print(\"2 Self-attention allows each word in a sequence to attend to (weigh) every other word in the same sequence, including itself. By computing query, key, and value vectors for each word, it captures contextual relationships, enabling the model to determine how much emphasis to place on different words when representing a specific word. This means the representation of a word isn't just its isolated meaning but also includes contributions from related words.\")\n",
        "\n",
        "\n",
        "# 5 question :\n",
        "# Import the pipeline function from the transformers library\n",
        "from transformers import pipeline\n",
        "\n",
        "def analyze_sentiment(sentence):\n",
        "    \"\"\"\n",
        "    Performs sentiment analysis on a given sentence using a pre-trained\n",
        "    HuggingFace sentiment analysis pipeline.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence to analyze.\n",
        "    \"\"\"\n",
        "    print(\"--- Sentiment Analysis using HuggingFace Transformers ---\")\n",
        "\n",
        "    # Load a pre-trained sentiment analysis pipeline\n",
        "    # The default model for 'sentiment-analysis' is usually 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "    # This model classifies text as POSITIVE or NEGATIVE.\n",
        "    try:\n",
        "        classifier = pipeline(\"sentiment-analysis\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading sentiment analysis pipeline: {e}\")\n",
        "        print(\"Please ensure you have an internet connection to download the model if it's not cached.\")\n",
        "        return\n",
        "\n",
        "    # Analyze the input sentence\n",
        "    result = classifier(sentence)[0] # The pipeline returns a list, we take the first (and only) item\n",
        "\n",
        "    # Print the label and confidence score\n",
        "    print(f\"Sentiment: {result['label']}\")\n",
        "    print(f\"Confidence Score: {result['score']:.4f}\") # Format to 4 decimal places\n",
        "\n",
        "# Input sentence for sentiment analysis\n",
        "input_sentence = \"Despite the high price, the performance of the new MacBook is outstanding.\"\n",
        "\n",
        "# Call the sentiment analysis function\n",
        "analyze_sentiment(input_sentence)\n",
        "\n",
        "\n",
        "# SHORT ANSWERS\n",
        "\n",
        "print(\"1, BERT (Bidirectional Encoder Representations from Transformers) uses a bidirectional Transformer encoder and is designed for understanding context from both left and right. GPT (Generative Pre-trained Transformer) uses a unidirectional Transformer decoder, making it ideal for generative tasks by predicting the next token based only on preceding tokens.\")\n",
        "\n",
        "print(\"2, Using pre-trained models is beneficial because they have already learned vast linguistic knowledge (grammar, semantics, world facts) from massive text datasets. This significantly reduces the need for large, labeled datasets and extensive computational resources for specific tasks. It allows for faster development and often achieves higher performance through fine-tuning, as the model starts with a strong understanding of language.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}